# Contributions

本文实现了两种新的减少节点的方法：合并和删除。合并是将同层内类型相同的节点合并，删除是用节点上界或下界代替节点变量。

本文实现了两种新的的精化方法：分裂和恢复，作为合并与删除的逆操作。使用抽象过程中的依赖图来限制精化顺序，以保证精化恢复网络的正确性。

# Overview

预处理将输入网络的节点分为 inc 和 dec 两类。可以假设之后的抽象和精化中所有的隐藏层节点都已经被分为 inc 和 dec 节点。

节点抽象包括 merge 和 freeze，保证 inc 节点的值增加以及 dec 节点的值减少，因此确保输出值增大。

精化包括 recover 和 split，split 把一个抽象节点分成原来的两个节点；recover 恢复被删除的节点。

随意地叠加 split 和 recover 不能保证得到上近似网络，因此它们按照依赖图的约束进行。

精化用于排除 spurious 反例，本文根据 cex 计算一种 profit function 来衡量对不同节点的精化效果。

# Abstraction

## Preprocessing

所有节点被分成 inc 或是 dec，其中 inc 表示它的值的增加将会导致输出的增加；dec 表示值的减小将会导致输出的减小。

为什么不是所有的节点都能分类为 inc 或 dec？因为不一定是单调的。

节点 $v_{i,j}$ 被分割为 $v_{+}$ 和 $v_{-}$ 后，$v_{+}$ 保留连向下一层正节点的正权重、连向下一层节点的负权重。

## Abstraction

Merge 把两种同类型的节点合并。当两个节点权重相差较大时，merge 误差较大，因此可用 freeze 把节点删去。

Propagate 用于 freeze 之后，将常数节点 propagate 到后一层节点，然后删去这一常数节点。

Propagate 作为单独的操作而不是和 freeze 合并，是因为：

+ 可以在 verify 时再一次调用 propagate；
+ 抽象网络 $N_{i}$ 的节点上下界可能互不相同（由于抽象），为了避免每次 freeze 之前计算一次上下界。

所以 generalized freeze 不能用于 merge 创建的节点，因为它们的上下界在原始网络中不存在。

但这样的限制又太严格，实际上 $N_j$ 即使不是 $N_{i}$ 的上近似，也是原始网络 $N$ 的上近似。

性质 4.12 保证 freeze 的可交换性。

Q：什么时候选 merge 什么时候选 freeze？

对应 4.4 节：选择哪些节点抽象，抽象到什么程度。

一个节点的重要性由它对输出的贡献确定：如果一个节点的数值上下界均值越大，认为它对输出的影响越大。以此找到重要性最小的节点。

然后是确定用 merge 还是 freeze，这通过衡量其 loss of accuracy 确定：freeze 用节点估计值（上下界均值）与 freeze 的常数差值；merge 用一种关于两个节点估计值的线性组合。两种 loss 在一起比较。

关于抽象多少步：选出一些满足性质的输入，抽象到一些输入不再满足给定性质。

# Refinement

不能选择任何一个抽象节点精化，因为抽象节点的选择会影响得到得到的抽象网络。因为这里的精化完全是抽象的逆操作，所以可能会导致无法复原到原网络。

依赖图是关于一个抽象序列的。存在的依赖关系有：

+ 第 $k$ 层节点的抽象依赖其后所有 $k'$ 层节点的 freeze；
+ merge 依赖所有产生被 merge 的同类节点的抽象，以及前一层、后一层的 merge 操作。

个人感觉最大的问题是需要依赖图这个东西，增加了方法的复杂度同时限制了精化能力。但是不限制精化顺序的话，直觉上又回不到原网络。

因此，在抽象的过程中，需要维护依赖图以帮助精化。

与抽象相反，我们期望一步精化能够恢复尽可能多的精度。这通过 spurious cex 衡量：通过输入 cex 下 abstraction 前节点的值与 abstraction 后节点的值的差。

NARv 使用 ReluVal 计算数值上下界。

# Comments

因为依赖图限制精化顺序，那么在实际实验中，有多少精化节点可选？


